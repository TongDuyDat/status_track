# track_status

H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng (ti·∫øng Vi·ªát)

## M·ª•c ƒë√≠ch
Project n√†y ch·ª©a m·ªôt pipeline x·ª≠ l√Ω ·∫£nh/ocr ƒë∆∞·ª£c t·ªëi ∆∞u cho GPU v·ªõi ch·∫ø ƒë·ªô "staged pipeline" (pipelined stages) v√† c√°c worker x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô. Bao g·ªìm c·∫£ API (FastAPI) ƒë·ªÉ upload/monitoring, c√°c pipeline x·ª≠ l√Ω vƒÉn b·∫£n/nh·∫≠n di·ªán, v√† scripts ƒë·ªÉ kh·ªüi ƒë·ªông worker t·ªëi ∆∞u.

## Y√™u c·∫ßu c∆° b·∫£n
- **Anaconda/Miniconda**: khuy·∫øn ngh·ªã ƒë·ªÉ qu·∫£n l√Ω m√¥i tr∆∞·ªùng Python
- **Python 3.8+** (khuy·∫øn ngh·ªã 3.9/3.10)
- **GPU + CUDA** n·∫øu mu·ªën ch·∫°y ONNX / GPU-accelerated runtime
- **Redis Server**: ƒë·ªÉ qu·∫£n l√Ω task queue v√† l∆∞u k·∫øt qu·∫£
- M·ªôt s·ªë th∆∞ vi·ªán th∆∞·ªùng th·∫•y trong project: FastAPI, uvicorn, onnxruntime (ho·∫∑c onnxruntime-gpu), numpy, redis, python-dotenv, asyncio, aiohttp

## C√†i ƒë·∫∑t Redis tr√™n Windows

### C√°ch 1: D√πng WSL2 (khuy·∫øn ngh·ªã)
```powershell
# Trong WSL2 Ubuntu
sudo apt update
sudo apt install redis-server

# Kh·ªüi ƒë·ªông Redis
sudo service redis-server start

# Ki·ªÉm tra Redis ƒëang ch·∫°y
redis-cli ping
# K·∫øt qu·∫£: PONG
```

### C√°ch 2: D√πng Redis for Windows (community port)
1. T·∫£i Redis for Windows t·ª´: https://github.com/tporadowski/redis/releases
2. Gi·∫£i n√©n v√† ch·∫°y `redis-server.exe`
3. Ki·ªÉm tra b·∫±ng `redis-cli.exe ping`

### C√°ch 3: D√πng Docker (d·ªÖ nh·∫•t)
```powershell
# Pull v√† ch·∫°y Redis container
docker run -d -p 6379:6379 --name redis redis:latest

# Ki·ªÉm tra
docker exec -it redis redis-cli ping
# K·∫øt qu·∫£: PONG
```

Sau khi c√†i ƒë·∫∑t, Redis s·∫Ω ch·∫°y t·∫°i `localhost:6379` (m·∫∑c ƒë·ªãnh).

## Chu·∫©n b·ªã m√¥i tr∆∞·ªùng Python v·ªõi Conda (Windows PowerShell)

### 1. T·∫°o conda environment m·ªõi
```powershell
# T·∫°o environment v·ªõi Python 3.10
conda create -n track_status python=3.10 -y

# K√≠ch ho·∫°t environment
conda activate track_status
```

### 2. C√†i ƒë·∫∑t ph·ª• thu·ªôc c∆° b·∫£n
N·∫øu repository c√≥ `requirements.txt`:

```powershell
pip install -r requirements.txt
```

N·∫øu kh√¥ng c√≥ `requirements.txt`, c√†i c√°c package c∆° b·∫£n:

```powershell
# Web framework & async
pip install fastapi uvicorn python-dotenv

# Redis client
pip install redis aioredis

# Data processing
pip install numpy pillow

# ONNX Runtime (CPU version)
pip install onnxruntime

# Ho·∫∑c GPU version (c·∫ßn CUDA ƒë√£ c√†i)
pip install onnxruntime-gpu
```

### 3. C√†i ƒë·∫∑t CUDA & cuDNN (n·∫øu d√πng GPU)
- C√†i CUDA Toolkit ph√π h·ª£p v·ªõi phi√™n b·∫£n onnxruntime-gpu
- Tham kh·∫£o: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html

L∆∞u √Ω: N·∫øu mu·ªën s·ª≠ d·ª•ng GPU v·ªõi onnxruntime, c√†i `onnxruntime-gpu` thay cho `onnxruntime` v√† ƒë·∫£m b·∫£o driver/CUDA t∆∞∆°ng th√≠ch.

## Bi·∫øn m√¥i tr∆∞·ªùng quan tr·ªçng

### T·∫°o file `.env` t·∫°i th∆∞ m·ª•c g·ªëc project
Copy t·ª´ file m·∫´u v√† ch·ªânh s·ª≠a theo nhu c·∫ßu:

```powershell
# Copy file m·∫´u
Copy-Item .env.example .env

# Ch·ªânh s·ª≠a file .env b·∫±ng text editor
notepad .env
```

N·ªôi dung tham kh·∫£o (xem chi ti·∫øt trong `.env.example`):

```env
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Pipeline Configuration
PIPELINE_MODE=staged
WORKER_BATCH_SIZE=16
BATCH_TIMEOUT=0.05
MAX_CONCURRENT_BATCHES=10

# Logging
LOG_LEVEL=INFO

# Memory Thresholds
RAM_THRESHOLD=0.85
GPU_THRESHOLD=0.90
MAX_QUEUE_SIZE=1000
```

### Ho·∫∑c set tr·ª±c ti·∫øp trong PowerShell (t·∫°m th·ªùi)
```powershell
# Redis
$env:REDIS_HOST = "localhost"
$env:REDIS_PORT = "6379"
$env:REDIS_DB = "0"

# Pipeline
$env:PIPELINE_MODE = "staged"
$env:WORKER_BATCH_SIZE = "16"
$env:BATCH_TIMEOUT = "0.05"
$env:MAX_CONCURRENT_BATCHES = "10"
$env:LOG_LEVEL = "INFO"
```

### Gi·∫£i th√≠ch c√°c bi·∫øn quan tr·ªçng:

**Redis:**
- `REDIS_HOST`: ƒë·ªãa ch·ªâ Redis server (m·∫∑c ƒë·ªãnh: localhost)
- `REDIS_PORT`: port Redis (m·∫∑c ƒë·ªãnh: 6379)
- `REDIS_DB`: database number (m·∫∑c ƒë·ªãnh: 0)
- `REDIS_PASSWORD`: password (ƒë·ªÉ tr·ªëng n·∫øu kh√¥ng d√πng auth)

**Pipeline:**
- `PIPELINE_MODE`: `staged` ƒë·ªÉ b·∫≠t ch·∫ø ƒë·ªô pipeline nhi·ªÅu stage (khuy·∫øn ngh·ªã)
- `WORKER_BATCH_SIZE`: s·ªë ·∫£nh t·ªëi ƒëa gom v√†o 1 batch x·ª≠ l√Ω tr√™n GPU (m·∫∑c ƒë·ªãnh: 16)
- `BATCH_TIMEOUT`: th·ªùi gian ch·ªù (gi√¢y) ƒë·ªÉ gom batch (m·∫∑c ƒë·ªãnh: 0.05 = 50ms)
- `MAX_CONCURRENT_BATCHES`: s·ªë batch ch·∫°y ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 10)
- `LOG_LEVEL`: m·ª©c ƒë·ªô log `INFO`/`DEBUG`

## Ch·∫°y worker / pipeline

**Quan tr·ªçng:** ƒê·∫£m b·∫£o Redis ƒë√£ ch·∫°y tr∆∞·ªõc khi kh·ªüi ƒë·ªông worker!

Ki·ªÉm tra Redis:
```powershell
# Test k·∫øt n·ªëi Redis
redis-cli ping
# Ho·∫∑c n·∫øu d√πng Docker:
docker exec -it redis redis-cli ping
```

### Kh·ªüi ƒë·ªông worker v·ªõi staged pipeline (khuy·∫øn ngh·ªã)

```powershell
# K√≠ch ho·∫°t conda environment
conda activate track_status

# Ch·∫°y worker (script n√†y t·ª± set c√°c env variables t·ªëi ∆∞u)
python .\start_staged_worker.py
```

Script `start_staged_worker.py` s·∫Ω t·ª± ƒë·ªông:
- Set `PIPELINE_MODE=staged`
- TƒÉng batch size l√™n 16 ƒë·ªÉ t·ªëi ∆∞u GPU
- C·∫•u h√¨nh c√°c th√¥ng s·ªë batch timeout v√† concurrent batches

### C√°c c√°ch ch·∫°y kh√°c

```powershell
# Ho·∫∑c d√πng main.py
python .\main.py

# Ho·∫∑c worker optimized
python .\start_worker_optimized.py
```

M·ªói script c√≥ th·ªÉ ch·ª©a c·∫•u h√¨nh kh√°c nhau; h√£y m·ªü file t∆∞∆°ng ·ª©ng ƒë·ªÉ xem c√°c th√¥ng s·ªë c·ª• th·ªÉ.

### Nh·ªØng g√¨ b·∫°n n√™n th·∫•y khi worker kh·ªüi ƒë·ªông th√†nh c√¥ng:
```
‚úÖ 'üöÄ Pipeline scheduler started with 3 stages'
‚úÖ 'üü¢ Stage 1/2/3 worker started'
‚úÖ '[BatchManager] Processing batch_size=16+' (kh√¥ng ph·∫£i 1!)
‚úÖ GPU utilization 80%+ (ki·ªÉm tra v·ªõi: python monitor_gpu.py)
```

## Ch·∫°y API (FastAPI)
Project c√≥ ph·∫ßn API ƒë·ªÉ upload ·∫£nh/ƒëa lu·ªìng v√† monitoring. ƒê·ªÉ ch·∫°y API development server (n·∫øu file ch·ª©a `app = FastAPI(...)` n·∫±m trong m·ªôt module):

1. T√¨m file ƒë·ªãnh nghƒ©a `app = FastAPI(...)` (v√≠ d·ª• `api/app.py` ho·∫∑c `api/__init__.py`).
2. Ch·∫°y uvicorn, v√≠ d·ª•:

```powershell
uvicorn api.app:app --reload
```

(L∆∞u √Ω: ƒëi·ªÅu ch·ªânh `api.app` th√†nh module ƒë√∫ng ch·ª©a `app` trong d·ª± √°n.)

## Monitor & ki·ªÉm tra GPU
- C√≥ script ti·ªán √≠ch:

```powershell
python .\monitor_gpu.py
```

- C√≥ tests li√™n quan GPU v√† hi·ªáu nƒÉng trong th∆∞ m·ª•c `tests/` (v√≠ d·ª• `tests/test_gpu_load.py`). Ch·∫°y test:

```powershell
python -m pytest -q tests/
```

## Ki·ªÉm th·ª≠ nhanh (smoke tests)
- D√πng c√°c test c√≥ s·∫µn trong `tests/` ƒë·ªÉ ki·ªÉm ch·ª©ng pipeline v√† GPU load quick tests.

## File/Th∆∞ m·ª•c quan tr·ªçng
- `main.py` ‚Äî quick-start cho staged pipeline (c·∫•u h√¨nh env v√† ch·∫°y worker)
- `start_staged_worker.py`, `start_worker_optimized.py` ‚Äî scripts kh·ªüi ƒë·ªông worker v·ªõi c√°c c·∫•u h√¨nh kh√°c nhau
- `worker/image_processor.py` ‚Äî logic x·ª≠ l√Ω ·∫£nh ch√≠nh
- `pipelines/` ‚Äî ch·ª©a c√°c pipeline (text detection/recognition, tracking, v.v)
- `api/` ‚Äî routes cho upload v√† monitoring
- `docs/` ‚Äî t√†i li·ªáu n·ªôi b·ªô, v√≠ d·ª• `ONNX_GPU_FIX.md`, `GPU_OPTIMIZATION.md` (tham kh·∫£o n·∫øu g·∫∑p l·ªói GPU/ONNX)
- `tools/` ‚Äî scripts ti·ªán √≠ch (v√≠ d·ª• `monitor_memory.py`, `debug_pipeline.py`)

## V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p & g·ª£i √Ω kh·∫Øc ph·ª•c

### Redis
- **L·ªói `ConnectionError: Error 10061`**: Redis ch∆∞a ch·∫°y. Kh·ªüi ƒë·ªông Redis server tr∆∞·ªõc.
- **L·ªói `WRONGPASS invalid username-password pair`**: Sai password Redis. Ki·ªÉm tra `REDIS_PASSWORD` trong `.env`.
- **L·ªói connection timeout**: Ki·ªÉm tra `REDIS_HOST` v√† `REDIS_PORT` c√≥ ƒë√∫ng kh√¥ng.

### Python Environment
- **L·ªói kh√¥ng t√¨m th·∫•y module/thi·∫øu package**: 
  - Ki·ªÉm tra conda environment ƒë√£ active ch∆∞a: `conda activate track_status`
  - C√†i l·∫°i dependencies: `pip install -r requirements.txt`
- **Import error**: ƒê·∫£m b·∫£o ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc project (n∆°i c√≥ `main.py`)

### ONNX & GPU
- **ONNX ch·∫°y ch·∫≠m ho·∫∑c g·∫∑p l·ªói CUDA**: 
  - Xem `docs/ONNX_GPU_FIX.md` ƒë·ªÉ bi·∫øt c√°c fix v√† flags khuy·∫øn ngh·ªã
  - Ki·ªÉm tra version CUDA t∆∞∆°ng th√≠ch v·ªõi onnxruntime-gpu
- **GPU out-of-memory**: 
  - Gi·∫£m `WORKER_BATCH_SIZE` (th·ª≠ 8 ho·∫∑c 4)
  - B·∫≠t mixed precision n·∫øu pipeline h·ªó tr·ª£ FP16
  - Ki·ªÉm tra GPU memory: `python monitor_gpu.py`

### API
- **API kh√¥ng kh·ªüi ƒë·ªông**: 
  - Ki·ªÉm tra file n∆°i `app = FastAPI(...)` 
  - Ch·∫°y `uvicorn` v·ªõi module path ch√≠nh x√°c
  - Ki·ªÉm tra port c√≥ b·ªã chi·∫øm kh√¥ng: `netstat -ano | findstr :8000`

## G·ª£i √Ω ph√°t tri·ªÉn ti·∫øp / next steps
- T·∫°o `requirements.txt` ch√≠nh x√°c cho d·ª± √°n (pip freeze t·ª´ m√¥i tr∆∞·ªùng dev). ƒêi·ªÅu n√†y gi√∫p c√†i ƒë·∫∑t reproducible.
- Th√™m `.env.example` v·ªõi bi·∫øn m√¥i tr∆∞·ªùng ph·ªï bi·∫øn.
- Th√™m Dockerfile / docker-compose cho tri·ªÉn khai production.
- Vi·∫øt pipeline-level integration tests v√† CI ƒë·ªÉ ki·ªÉm tra hi·ªáu nƒÉng GPU.

## Li√™n k·∫øt tham kh·∫£o n·ªôi b·ªô
- `docs/GPU_OPTIMIZATION.md`
- `docs/ONNX_GPU_FIX.md`
- `docs/STAGED_PIPELINE_OPTIMIZATION.md`

---
N·∫øu b·∫°n mu·ªën, t√¥i c√≥ th·ªÉ:
- T·∫°o `requirements.txt` m·∫´u b·∫±ng c√°ch qu√©t imports trong code.
- T·∫°o file `.env.example` v·ªõi c√°c bi·∫øn m√¥i tr∆∞·ªùng th∆∞·ªùng d√πng.
- Th√™m v√≠ d·ª• ch·∫°y `uvicorn` ch√≠nh x√°c n·∫øu b·∫°n cho bi·∫øt file ch·ª©a `app = FastAPI(...)`.

C·∫ßn m√¨nh ch·ªânh n·ªôi dung README (b·ªï sung chi ti·∫øt file, l·ªánh c·ª• th·ªÉ) theo √Ω b·∫°n ch·ªó n√†o kh√¥ng?"# status_track" 
