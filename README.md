# track_status

H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng (ti·∫øng Vi·ªát)

## M·ª•c ƒë√≠ch
Project n√†y ch·ª©a m·ªôt pipeline x·ª≠ l√Ω ·∫£nh/ocr ƒë∆∞·ª£c t·ªëi ∆∞u cho GPU v·ªõi ch·∫ø ƒë·ªô "staged pipeline" (pipelined stages) v√† c√°c worker x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô. Bao g·ªìm c·∫£ API (FastAPI) ƒë·ªÉ upload/monitoring, c√°c pipeline x·ª≠ l√Ω vƒÉn b·∫£n/nh·∫≠n di·ªán, v√† scripts ƒë·ªÉ kh·ªüi ƒë·ªông worker t·ªëi ∆∞u.

## C√°c c√°ch c√†i ƒë·∫∑t

Project h·ªó tr·ª£ 2 c√°ch c√†i ƒë·∫∑t:

### üê≥ C√°ch 1: Docker (Khuy·∫øn ngh·ªã - D·ªÖ nh·∫•t)
- ‚úÖ Kh√¥ng c·∫ßn c√†i ƒë·∫∑t Python, CUDA, dependencies
- ‚úÖ T·ª± ƒë·ªông c√†i Redis
- ‚úÖ M√¥i tr∆∞·ªùng chu·∫©n, reproducible
- ‚úÖ D·ªÖ deploy v√† scale
- üëâ [Xem h∆∞·ªõng d·∫´n Docker](#c√†i-ƒë·∫∑t-v·ªõi-docker)

### üêç C√°ch 2: Conda (Development)
- ‚úÖ Ph√π h·ª£p cho development v√† debug
- ‚úÖ C√≥ th·ªÉ custom dependencies
- ‚ö†Ô∏è C·∫ßn c√†i CUDA, Redis ri√™ng
- üëâ [Xem h∆∞·ªõng d·∫´n Conda](#c√†i-ƒë·∫∑t-v·ªõi-conda)

---

## üê≥ C√†i ƒë·∫∑t v·ªõi Docker

### Y√™u c·∫ßu
- **Docker Desktop** (Windows/Mac) ho·∫∑c Docker Engine (Linux)
- **NVIDIA Docker runtime** (cho GPU support)
  - Windows: Docker Desktop v·ªõi WSL2 + NVIDIA Container Toolkit
  - Linux: nvidia-docker2

### C√†i ƒë·∫∑t NVIDIA Container Toolkit (cho GPU)

#### Windows v·ªõi WSL2:
```powershell
# Trong WSL2 Ubuntu
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

#### Linux:
```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Kh·ªüi ƒë·ªông project v·ªõi Docker Compose

```powershell
# 1. Clone repository (n·∫øu ch∆∞a c√≥)
git clone <repository-url>
cd track_status

# 2. T·∫°o file .env t·ª´ template
Copy-Item .env.example .env

# 3. Build v√† kh·ªüi ƒë·ªông t·∫•t c·∫£ services (Redis + Worker)
docker-compose up -d

# 4. Xem logs
docker-compose logs -f worker

# 5. Ki·ªÉm tra status
docker-compose ps
```

### C√°c l·ªánh Docker h·ªØu √≠ch

```powershell
# D·ª´ng t·∫•t c·∫£ services
docker-compose down

# Rebuild image khi c√≥ thay ƒë·ªïi code
docker-compose up -d --build

# Xem logs c·ªßa worker
docker-compose logs -f worker

# Xem logs c·ªßa Redis
docker-compose logs -f redis

# Ch·∫°y ch·ªâ Redis
docker-compose up -d redis

# Ch·∫°y ch·ªâ Worker (y√™u c·∫ßu Redis ƒë√£ ch·∫°y)
docker-compose up -d worker

# Ch·∫°y API service
docker-compose up -d api

# V√†o terminal c·ªßa worker container
docker-compose exec worker bash

# Ki·ªÉm tra GPU trong container
docker-compose exec worker nvidia-smi

# Restart worker
docker-compose restart worker

# X√≥a t·∫•t c·∫£ (bao g·ªìm volumes)
docker-compose down -v
```

### C·∫•u h√¨nh Docker

Ch·ªânh s·ª≠a file `docker-compose.yml` ho·∫∑c `.env` ƒë·ªÉ thay ƒë·ªïi c·∫•u h√¨nh:

```env
# .env file
REDIS_PORT=6379
API_PORT=8000
WORKER_BATCH_SIZE=16
LOG_LEVEL=INFO
```

### Monitor GPU trong Docker

```powershell
# Xem GPU usage t·ª´ host
nvidia-smi

# Xem GPU usage trong container
docker-compose exec worker python monitor_gpu.py
```

---

## üêç C√†i ƒë·∫∑t v·ªõi Conda

### Y√™u c·∫ßu c∆° b·∫£n
- **Anaconda/Miniconda**: khuy·∫øn ngh·ªã ƒë·ªÉ qu·∫£n l√Ω m√¥i tr∆∞·ªùng Python
- **Python 3.8+** (khuy·∫øn ngh·ªã 3.9/3.10)
- **GPU + CUDA** n·∫øu mu·ªën ch·∫°y ONNX / GPU-accelerated runtime
- **Redis Server**: ƒë·ªÉ qu·∫£n l√Ω task queue v√† l∆∞u k·∫øt qu·∫£
- M·ªôt s·ªë th∆∞ vi·ªán th∆∞·ªùng th·∫•y trong project: FastAPI, uvicorn, onnxruntime (ho·∫∑c onnxruntime-gpu), numpy, redis, python-dotenv, asyncio, aiohttp

## C√†i ƒë·∫∑t Redis tr√™n Windows

### C√°ch 1: D√πng WSL2 (khuy·∫øn ngh·ªã)
```powershell
# Trong WSL2 Ubuntu
sudo apt update
sudo apt install redis-server

# Kh·ªüi ƒë·ªông Redis
sudo service redis-server start

# Ki·ªÉm tra Redis ƒëang ch·∫°y
redis-cli ping
# K·∫øt qu·∫£: PONG
```

### C√°ch 2: D√πng Redis for Windows (community port)
1. T·∫£i Redis for Windows t·ª´: https://github.com/tporadowski/redis/releases
2. Gi·∫£i n√©n v√† ch·∫°y `redis-server.exe`
3. Ki·ªÉm tra b·∫±ng `redis-cli.exe ping`

### C√°ch 3: D√πng Docker (d·ªÖ nh·∫•t)
```powershell
# Pull v√† ch·∫°y Redis container
docker run -d -p 6379:6379 --name redis redis:latest

# Ki·ªÉm tra
docker exec -it redis redis-cli ping
# K·∫øt qu·∫£: PONG
```

Sau khi c√†i ƒë·∫∑t, Redis s·∫Ω ch·∫°y t·∫°i `localhost:6379` (m·∫∑c ƒë·ªãnh).

## Chu·∫©n b·ªã m√¥i tr∆∞·ªùng Python v·ªõi Conda (Windows PowerShell)

### 1. T·∫°o conda environment m·ªõi
```powershell
# T·∫°o environment v·ªõi Python 3.10
conda create -n track_status python=3.10 -y

# K√≠ch ho·∫°t environment
conda activate track_status
```

### 2. C√†i ƒë·∫∑t ph·ª• thu·ªôc c∆° b·∫£n
N·∫øu repository c√≥ `requirements.txt`:

```powershell
pip install -r requirements.txt
```

N·∫øu kh√¥ng c√≥ `requirements.txt`, c√†i c√°c package c∆° b·∫£n:

```powershell
# Web framework & async
pip install fastapi uvicorn python-dotenv

# Redis client
pip install redis aioredis

# Data processing
pip install numpy pillow

# ONNX Runtime (CPU version)
pip install onnxruntime

# Ho·∫∑c GPU version (c·∫ßn CUDA ƒë√£ c√†i)
pip install onnxruntime-gpu
```

### 3. C√†i ƒë·∫∑t CUDA & cuDNN (n·∫øu d√πng GPU)
- C√†i CUDA Toolkit ph√π h·ª£p v·ªõi phi√™n b·∫£n onnxruntime-gpu
- Tham kh·∫£o: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html

L∆∞u √Ω: N·∫øu mu·ªën s·ª≠ d·ª•ng GPU v·ªõi onnxruntime, c√†i `onnxruntime-gpu` thay cho `onnxruntime` v√† ƒë·∫£m b·∫£o driver/CUDA t∆∞∆°ng th√≠ch.

## Bi·∫øn m√¥i tr∆∞·ªùng quan tr·ªçng

### T·∫°o file `.env` t·∫°i th∆∞ m·ª•c g·ªëc project
Copy t·ª´ file m·∫´u v√† ch·ªânh s·ª≠a theo nhu c·∫ßu:

```powershell
# Copy file m·∫´u
Copy-Item .env.example .env

# Ch·ªânh s·ª≠a file .env b·∫±ng text editor
notepad .env
```

N·ªôi dung tham kh·∫£o (xem chi ti·∫øt trong `.env.example`):

```env
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Pipeline Configuration
PIPELINE_MODE=staged
WORKER_BATCH_SIZE=16
BATCH_TIMEOUT=0.05
MAX_CONCURRENT_BATCHES=10

# Logging
LOG_LEVEL=INFO

# Memory Thresholds
RAM_THRESHOLD=0.85
GPU_THRESHOLD=0.90
MAX_QUEUE_SIZE=1000
```

### Ho·∫∑c set tr·ª±c ti·∫øp trong PowerShell (t·∫°m th·ªùi)
```powershell
# Redis
$env:REDIS_HOST = "localhost"
$env:REDIS_PORT = "6379"
$env:REDIS_DB = "0"

# Pipeline
$env:PIPELINE_MODE = "staged"
$env:WORKER_BATCH_SIZE = "16"
$env:BATCH_TIMEOUT = "0.05"
$env:MAX_CONCURRENT_BATCHES = "10"
$env:LOG_LEVEL = "INFO"
```

### Gi·∫£i th√≠ch c√°c bi·∫øn quan tr·ªçng:

**Redis:**
- `REDIS_HOST`: ƒë·ªãa ch·ªâ Redis server (m·∫∑c ƒë·ªãnh: localhost)
- `REDIS_PORT`: port Redis (m·∫∑c ƒë·ªãnh: 6379)
- `REDIS_DB`: database number (m·∫∑c ƒë·ªãnh: 0)
- `REDIS_PASSWORD`: password (ƒë·ªÉ tr·ªëng n·∫øu kh√¥ng d√πng auth)

**Pipeline:**
- `PIPELINE_MODE`: `staged` ƒë·ªÉ b·∫≠t ch·∫ø ƒë·ªô pipeline nhi·ªÅu stage (khuy·∫øn ngh·ªã)
- `WORKER_BATCH_SIZE`: s·ªë ·∫£nh t·ªëi ƒëa gom v√†o 1 batch x·ª≠ l√Ω tr√™n GPU (m·∫∑c ƒë·ªãnh: 16)
- `BATCH_TIMEOUT`: th·ªùi gian ch·ªù (gi√¢y) ƒë·ªÉ gom batch (m·∫∑c ƒë·ªãnh: 0.05 = 50ms)
- `MAX_CONCURRENT_BATCHES`: s·ªë batch ch·∫°y ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 10)
- `LOG_LEVEL`: m·ª©c ƒë·ªô log `INFO`/`DEBUG`

## Ch·∫°y worker / pipeline

**Quan tr·ªçng:** ƒê·∫£m b·∫£o Redis ƒë√£ ch·∫°y tr∆∞·ªõc khi kh·ªüi ƒë·ªông worker!

Ki·ªÉm tra Redis:
```powershell
# Test k·∫øt n·ªëi Redis
redis-cli ping
# Ho·∫∑c n·∫øu d√πng Docker:
docker exec -it redis redis-cli ping
```

### Kh·ªüi ƒë·ªông worker v·ªõi staged pipeline (khuy·∫øn ngh·ªã)

```powershell
# K√≠ch ho·∫°t conda environment
conda activate track_status

# Ch·∫°y worker (script n√†y t·ª± set c√°c env variables t·ªëi ∆∞u)
python .\start_staged_worker.py
```

Script `start_staged_worker.py` s·∫Ω t·ª± ƒë·ªông:
- Set `PIPELINE_MODE=staged`
- TƒÉng batch size l√™n 16 ƒë·ªÉ t·ªëi ∆∞u GPU
- C·∫•u h√¨nh c√°c th√¥ng s·ªë batch timeout v√† concurrent batches

### C√°c c√°ch ch·∫°y kh√°c

```powershell
# Ho·∫∑c d√πng main.py
python .\main.py

# Ho·∫∑c worker optimized
python .\start_worker_optimized.py
```

M·ªói script c√≥ th·ªÉ ch·ª©a c·∫•u h√¨nh kh√°c nhau; h√£y m·ªü file t∆∞∆°ng ·ª©ng ƒë·ªÉ xem c√°c th√¥ng s·ªë c·ª• th·ªÉ.

### Nh·ªØng g√¨ b·∫°n n√™n th·∫•y khi worker kh·ªüi ƒë·ªông th√†nh c√¥ng:
```
‚úÖ 'üöÄ Pipeline scheduler started with 3 stages'
‚úÖ 'üü¢ Stage 1/2/3 worker started'
‚úÖ '[BatchManager] Processing batch_size=16+' (kh√¥ng ph·∫£i 1!)
‚úÖ GPU utilization 80%+ (ki·ªÉm tra v·ªõi: python monitor_gpu.py)
```

## Ch·∫°y API (FastAPI)
Project c√≥ ph·∫ßn API ƒë·ªÉ upload ·∫£nh/ƒëa lu·ªìng v√† monitoring. ƒê·ªÉ ch·∫°y API development server (n·∫øu file ch·ª©a `app = FastAPI(...)` n·∫±m trong m·ªôt module):

1. T√¨m file ƒë·ªãnh nghƒ©a `app = FastAPI(...)` (v√≠ d·ª• `api/app.py` ho·∫∑c `api/__init__.py`).
2. Ch·∫°y uvicorn, v√≠ d·ª•:

```powershell
uvicorn api.app:app --reload
```

(L∆∞u √Ω: ƒëi·ªÅu ch·ªânh `api.app` th√†nh module ƒë√∫ng ch·ª©a `app` trong d·ª± √°n.)

## Monitor & ki·ªÉm tra GPU
- C√≥ script ti·ªán √≠ch:

```powershell
python .\monitor_gpu.py
```

- C√≥ tests li√™n quan GPU v√† hi·ªáu nƒÉng trong th∆∞ m·ª•c `tests/` (v√≠ d·ª• `tests/test_gpu_load.py`). Ch·∫°y test:

```powershell
python -m pytest -q tests/
```

## Ki·ªÉm th·ª≠ nhanh (smoke tests)
- D√πng c√°c test c√≥ s·∫µn trong `tests/` ƒë·ªÉ ki·ªÉm ch·ª©ng pipeline v√† GPU load quick tests.

## File/Th∆∞ m·ª•c quan tr·ªçng
- `main.py` ‚Äî quick-start cho staged pipeline (c·∫•u h√¨nh env v√† ch·∫°y worker)
- `start_staged_worker.py`, `start_worker_optimized.py` ‚Äî scripts kh·ªüi ƒë·ªông worker v·ªõi c√°c c·∫•u h√¨nh kh√°c nhau
- `worker/image_processor.py` ‚Äî logic x·ª≠ l√Ω ·∫£nh ch√≠nh
- `pipelines/` ‚Äî ch·ª©a c√°c pipeline (text detection/recognition, tracking, v.v)
- `api/` ‚Äî routes cho upload v√† monitoring
- `docs/` ‚Äî t√†i li·ªáu n·ªôi b·ªô, v√≠ d·ª• `ONNX_GPU_FIX.md`, `GPU_OPTIMIZATION.md` (tham kh·∫£o n·∫øu g·∫∑p l·ªói GPU/ONNX)
- `tools/` ‚Äî scripts ti·ªán √≠ch (v√≠ d·ª• `monitor_memory.py`, `debug_pipeline.py`)

## V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p & g·ª£i √Ω kh·∫Øc ph·ª•c

### Docker

#### GPU kh√¥ng nh·∫≠n di·ªán trong container
```powershell
# Ki·ªÉm tra nvidia-docker ho·∫°t ƒë·ªông
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# N·∫øu l·ªói, restart Docker
# Windows: Restart Docker Desktop
# Linux: sudo systemctl restart docker

# Ki·ªÉm tra driver v√† runtime
nvidia-smi
docker info | grep -i runtime
```

#### Container kh√¥ng kh·ªüi ƒë·ªông
```powershell
# Xem logs chi ti·∫øt
docker-compose logs worker

# Rebuild image
docker-compose build --no-cache worker
docker-compose up -d worker

# Ki·ªÉm tra Redis ƒë√£ ch·∫°y ch∆∞a
docker-compose ps redis
docker-compose exec redis redis-cli ping
```

#### Out of memory trong container
```powershell
# Gi·∫£m batch size trong .env ho·∫∑c docker-compose.yml
WORKER_BATCH_SIZE=8
TRUCK_MAX_BATCH=16
TEXT_MAX_BATCH=8
OCR_MAX_BATCH=32

# Restart worker
docker-compose restart worker
```

### Redis
- **L·ªói `ConnectionError: Error 10061`**: Redis ch∆∞a ch·∫°y. Kh·ªüi ƒë·ªông Redis server tr∆∞·ªõc.
- **L·ªói `WRONGPASS invalid username-password pair`**: Sai password Redis. Ki·ªÉm tra `REDIS_PASSWORD` trong `.env`.
- **L·ªói connection timeout**: Ki·ªÉm tra `REDIS_HOST` v√† `REDIS_PORT` c√≥ ƒë√∫ng kh√¥ng.

### Python Environment
- **L·ªói kh√¥ng t√¨m th·∫•y module/thi·∫øu package**: 
  - Ki·ªÉm tra conda environment ƒë√£ active ch∆∞a: `conda activate track_status`
  - C√†i l·∫°i dependencies: `pip install -r requirements.txt`
- **Import error**: ƒê·∫£m b·∫£o ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc project (n∆°i c√≥ `main.py`)

### ONNX & GPU
- **ONNX ch·∫°y ch·∫≠m ho·∫∑c g·∫∑p l·ªói CUDA**: 
  - Xem `docs/ONNX_GPU_FIX.md` ƒë·ªÉ bi·∫øt c√°c fix v√† flags khuy·∫øn ngh·ªã
  - Ki·ªÉm tra version CUDA t∆∞∆°ng th√≠ch v·ªõi onnxruntime-gpu
- **GPU out-of-memory**: 
  - Gi·∫£m `WORKER_BATCH_SIZE` (th·ª≠ 8 ho·∫∑c 4)
  - B·∫≠t mixed precision n·∫øu pipeline h·ªó tr·ª£ FP16
  - Ki·ªÉm tra GPU memory: `python monitor_gpu.py`

### API
- **API kh√¥ng kh·ªüi ƒë·ªông**: 
  - Ki·ªÉm tra file n∆°i `app = FastAPI(...)` 
  - Ch·∫°y `uvicorn` v·ªõi module path ch√≠nh x√°c
  - Ki·ªÉm tra port c√≥ b·ªã chi·∫øm kh√¥ng: `netstat -ano | findstr :8000`

---

## üöÄ Quick Reference

### Docker Commands

```powershell
# Start everything
docker-compose up -d

# View logs
docker-compose logs -f worker

# Check status
docker-compose ps

# Stop everything
docker-compose down

# Rebuild after code changes
docker-compose up -d --build

# Check environment
.\check-docker-env.ps1
```

### Conda Commands

```powershell
# Activate environment
conda activate track_status

# Start worker
python .\start_staged_worker.py

# Monitor GPU
python .\monitor_gpu.py

# Run tests
python -m pytest tests/
```

## üìö T√†i li·ªáu b·ªï sung

- **[DOCKER.md](DOCKER.md)** - H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ Docker deployment
- **[docs/GPU_OPTIMIZATION.md](docs/GPU_OPTIMIZATION.md)** - T·ªëi ∆∞u GPU
- **[docs/ONNX_GPU_FIX.md](docs/ONNX_GPU_FIX.md)** - Fix ONNX GPU issues
- **[docs/STAGED_PIPELINE_OPTIMIZATION.md](docs/STAGED_PIPELINE_OPTIMIZATION.md)** - Pipeline optimization
- **[worker/README.md](worker/README.md)** - Worker documentation

## ü§ù ƒê√≥ng g√≥p

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ ho·∫∑c c√≥ ƒë·ªÅ xu·∫•t c·∫£i ti·∫øn:
1. T·∫°o issue m√¥ t·∫£ chi ti·∫øt
2. Fork repository
3. T·∫°o branch cho feature/fix
4. Submit pull request

## üìÑ License

[Th√™m license information n·∫øu c√≥]

---

**Repository**: [TongDuyDat/status_track](https://github.com/TongDuyDat/status_track)

C·∫ßn h·ªó tr·ª£? T·∫°o issue tr√™n GitHub ho·∫∑c li√™n h·ªá maintainer. 
